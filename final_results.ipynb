{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99ba5971",
   "metadata": {},
   "source": [
    "In my project I did topic modeling of movie conversations to cluster the movies and find new additinal ways of finding which movies are similer. \n",
    "\n",
    "The data set I used was from https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html, and it has conversations from the movie script of 617 different movies. It also has metadata for those movies, including their titles, genres, release year, and imdb rating. \n",
    "\n",
    "To do the topic modeling first I needed to clean the data. For this I used some code from https://github.com/sethns/Latent-Dirichlet-Allocation-LDA-/blob/main/Topic%20Modeling%20_%20Extracting%20Topics_%20Using%20Sklearn.ipynb, as well as some of professor Barsky's code here https://github.com/mgbarsky/research_pop_songs/blob/main/parse_lyrics.py, and some of my own code. Additinally many of the words had to be manually removed (they can be found in the list named \"bad\" in the Clean.py document).\n",
    "\n",
    "After cleaning the data I used sklearns LDA to do topic modeling. LDA stands for latent dirichlet allocation, and it is a way of finding hiden topics in a set of documents. It uses the Dirichlet distribution which can model which words are repeating together, are frequent, and are similar to each other. Its final result will be the probobility of each word being assigned to a given topic, and the probibility of a topic being assigned to a given document. \n",
    "\n",
    "For my final results I used five topics, as this gave the best movie-topic spread as well as perplexity score and t-SNE clustering. In the graph bellow we can see the movie-topic spread, and we see that although the fifth topic has a lot more movies assigned to it (with highest probability), all topics have a significant number of assigned movies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311d924b",
   "metadata": {},
   "source": [
    "<img src = \"final_results/movie-topic_count.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e375713",
   "metadata": {},
   "source": [
    "t-SNE is a way to reduce high dimention data to just 2-D, so that we can visualize them. \n",
    "As mentioned I used t-SNE on the topic modeling (with professor Barsky's code here https://github.com/mgbarsky/research_pop_songs). The t-SNE plot is shown bellow, and in it we can see that although there is some intermingling between clusters (like the blues and orange), there still is very clearly 5 different clusters that can be made. This indicates that the topic modeling was somewhat successful. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e122a0",
   "metadata": {},
   "source": [
    "<img src = \"final_results/t-SNE.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5146d8",
   "metadata": {},
   "source": [
    "I visulized the top 100 words for each of the topics as a word cloud. The size of a word is poportinal to its probability of being in the topic (multiplied by 100000 because I could not find an application that accepted floats as weight). The words, prbobilities, and weights used can be found in the csv files named \"topicsi\". The clouds were generated here https://wordart.com/ (taking a screenshot of my computer). \n",
    "\n",
    "# Word clouds:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745a9f5e",
   "metadata": {},
   "source": [
    "<img src = \"topic word clouds/topic1.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702b36fd",
   "metadata": {},
   "source": [
    "<img src = \"topic word clouds/topic2.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce8d856c",
   "metadata": {},
   "source": [
    "<img src = \"topic word clouds/topic3.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cfbdc50",
   "metadata": {},
   "source": [
    "<img src = \"topic word clouds/topic4.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de52e34e",
   "metadata": {},
   "source": [
    "<img src = \"topic word clouds/topic5.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c7b01f",
   "metadata": {},
   "source": [
    "# Clusteing:\n",
    "After modeling the topic we used it to make five movies clusters with sklearn's kmeans. But before doing this, we first made clusters based on the tf_idf scores of the movies.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "534b264e",
   "metadata": {},
   "source": [
    "The movies in each cluster can be found in the text file TFclusters.txt in the final_results folder. \n",
    "\n",
    "The clusturing had a SSE score of 559.4991636004921. \n",
    "\n",
    "To see how well it clusters, I made a heat map sorted by clusters. The heat map is split into the first two so my computer could render it. \n",
    "\n",
    "We see that the heat maps shows a lot of dark everywhere which makes sense becasue this all the tf_idf vlaues are between 0 and 1, and are typically very small. So we would expect the differences to be small. That being said we can see that this clustring doesn't really do a good job, as there is no clear clusters on the diaginal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80439ba7",
   "metadata": {},
   "source": [
    "### tf_idf clustering heatmap:\n",
    "First part:\n",
    "<img src = \"final_results/tf_clustering_1.png\" text-align=\"center\" width=50%>\n",
    "Second part:\n",
    "<img src = \"final_results/tf_clustering_2.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062a326",
   "metadata": {},
   "source": [
    "I then did clustering with both the tf_idr scores and the topic modeling scores (so I just added a few more dimentions). This clustering actually did a little worse in terms of the objective function with a SSE of 614.5566964334488.\n",
    "\n",
    "The movies in each cluster can be found in the text file clusters.txt in the final_results folder. \n",
    "\n",
    "Again I made a heat map for this clustering split into two parts. The heat map actually has more coherent black clusters. But it has them all over which again siginals a bad clusturing. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1c2430",
   "metadata": {},
   "source": [
    "### tf_idf and topic modeling clustering heatmap\n",
    "First part:\n",
    "<img src = \"final_results/all_clustering_1.png\" text-align=\"center\" width=50%>\n",
    "Second part:\n",
    "<img src = \"final_results/all_clustering_2.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ed354",
   "metadata": {},
   "source": [
    "I then did the clustering with just the topic modeling. Here the was a a very big reduction in the objective function, with SSE of 5.8923436624083. \n",
    "\n",
    "The movies for these clusters can be found in the text file Tclusters.txt in the final_results folder. \n",
    "\n",
    "Once again wr have a heat map for this clustering split into two parts. This heat map is almost entirely black which siginals that the heat map might not be the best measure to use here, as all the distinces are pretty small. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c34e47",
   "metadata": {},
   "source": [
    "### Just topic modeling cluster heatmap\n",
    "First part:\n",
    "<img src = \"final_results/ec_topic_clustering_1.png\" text-align=\"center\" width=50%>\n",
    "Second part:\n",
    "<img src = \"final_results/ec_topic_clustering_2.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842a4700",
   "metadata": {},
   "source": [
    "Finally I did another clustering with just the topic modeling but this time I used the cosine distince instead on euclidean distince. I used professor barsky's Kmean code for this because sklearn does not support non euclidean distinces. \n",
    "\n",
    "The movies for these clusters can be found in the text file clusters.txt in the final_results folder. \n",
    "\n",
    "Because I wasn't using sklearn I did not have an SSE score for this (I did not find the time to implement one myself). But I did make a heatmap for this clustering, shown in two parts bellow. As we can see this clustering clearly produces five coherent, that are significanly diffenrent then everything around them. \n",
    "\n",
    "This signifies that the right distince function to use all might have been cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6707e0",
   "metadata": {},
   "source": [
    "### Just topic heatmap using cosine\n",
    "First part:\n",
    "<img src = \"final_results/co_topic_clustering_1.png\" text-align=\"center\" width=50%>\n",
    "Second part:\n",
    "<img src = \"final_results/co_topic_clustering_2.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ba049d",
   "metadata": {},
   "source": [
    "Given this I redid the previos two clusters using the cosine distince. \n",
    "\n",
    "The movies for these clusters can be found in the text file cTFlusters.txt and cAllclusters.txt in the final_results folder. \n",
    "\n",
    "To judge them again I used heat maps shown bellow. In them we can see that even with cosine just using tf_idf does not produce any good clustring. Actually the clustering might even be worse, as it seems that almost none of the movies are similer to one another, with just one black strip down the diaginal. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd98666",
   "metadata": {},
   "source": [
    "### tf_idf clustering heatmap using cosine\n",
    "First part:\n",
    "<img src = \"final_results/co_tf_clustering_1.png\" text-align=\"center\" width=50%>\n",
    "Second part:\n",
    "<img src = \"final_results/co_tf_clustering_2.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f468de57",
   "metadata": {},
   "source": [
    "When using both tf_idf and the topic modeling there is a clear improvemnt in the clustering. There are at least four clear clusters on the diagonal, and a lot more distinction is happening. That being said even the clear clusters are not as dark as they should be, and a few other places that have dark clusters that are not on the diagonal. So while this is an improvement, overall the clustering that just uses the topic modeling and the cosine distince clearlly produces the best heatmap. So we can see that the topic modeling attributes is definitly an improvement over the just the word frequency. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a85db7",
   "metadata": {},
   "source": [
    "### tf_idf and topic modeling clustering heatmap using cosine\n",
    "First part:\n",
    "<img src = \"final_results/co_all_clustering_1.png\" text-align=\"center\" width=50%>\n",
    "Second part:\n",
    "<img src = \"final_results/co_all_clustering_2.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b803ae1",
   "metadata": {},
   "source": [
    "\n",
    "Finally I compared the clusters made from just the topic modeling to the known genres of the movies.The matrix bellow showes this where the x-axis are the clusters, and the y-axis are the genres.\n",
    "\n",
    "We wouldn't expect the clusters to fully cover the genres as they are an additional method of recognizing similarity not instead of genres. Surprisingly enough some of the genres such as crime, sci-fi, and thriller, were covered very well by this clustering. Other genres like drama and action were not coverd very well though. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed832e2",
   "metadata": {},
   "source": [
    "<img src = \"final_results/clusterVSground.png\" text-align=\"center\" width=50%>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54305d0f",
   "metadata": {},
   "source": [
    "This sums up the results of my project. Given more time I would work on the recommender and see if I could make that work given some other additional parameters. I would also do additinal satisitical comparesens between the different clusterings, that I did not have time for. \n",
    "\n",
    "Thank you very much professor Barsky for your much needed help and guidence in my project, and for all your code that you shared with me. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
